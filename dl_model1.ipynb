{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slingblade/anaconda3/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our first approach to using Deep Learning to predict the author of a text will be to use a shallow neural network \n",
    "#with non-linear activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Data/dataset.csv',names=['Author','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].map(lambda x: re.sub('\\r|\\n|\\'','',x))\n",
    "df['Text'] = df['Text'].map(lambda x: re.sub(r'--\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_subset = set([x for x in stop_words if 3 <= len(x) <= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_copy = df['Text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(x): #x is the string input\n",
    "    word_set = x.split(' ')\n",
    "    for word in word_set:\n",
    "        if word in stop_words:\n",
    "            word_set.remove(word)\n",
    "    return ' '.join(word_set)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "df['Text'] = df['Text'].map(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       U.S. Senators Tuesday sharply criticized new S...\n",
       "1       Two members Congress criticised Federal Reserv...\n",
       "2       Commuters stuck traffic Leesburg Pike Northern...\n",
       "3       A broad coalition corporations went Capitol Hi...\n",
       "4       On Internet, new products come go blink eye, t...\n",
       "5       Legislators continued debate Wednesday one dif...\n",
       "6       A top federal regulator Thursday urged banks b...\n",
       "7       Congress revives debate encryption export poli...\n",
       "8       Congress revives debate encryption export poli...\n",
       "9       Federal bank regulators begun prodding U.S. fi...\n",
       "10      Privacy advocates warned Wednesday Clinton adm...\n",
       "11      The number banks charging non-customers using ...\n",
       "12      The number banks charging non-customers using ...\n",
       "13      The rapidly evolving market stored-value cards...\n",
       "14      As new class financial derivative based credit...\n",
       "15      The merger Bankers Trust New York Corp. Alex. ...\n",
       "16      The merger Bankers Trust New York Corp. Alex. ...\n",
       "17      The U.S. government scrambling update thousand...\n",
       "18      There many places hang hat cyberspace soon, th...\n",
       "19      There may many places hang hat cyberspace soon...\n",
       "20      The largest online services providers reacted ...\n",
       "21      There many places hang hat cyberspace soon, th...\n",
       "22      The U.S. government scrambling update thousand...\n",
       "23      The Federal Reserve may taking adequate precau...\n",
       "24      The chairman an influential National Research ...\n",
       "25      A bill protect homeowners paying unneccessary ...\n",
       "26      U.S. Commerce Department forming committee adv...\n",
       "27      A bill dramatically relax U.S. export controls...\n",
       "28      Senator Bob Kerrey preparing legislation an at...\n",
       "29      Legislation dramatically relax U.S. export res...\n",
       "                              ...                        \n",
       "4970    China gave U.S. President Bill Clinton signal ...\n",
       "4971    China said Monday expected talks United States...\n",
       "4972    China said Monday expected talks United States...\n",
       "4973    European Commission Vice President Sir Leon Br...\n",
       "4974    China said Sunday United States trying contain...\n",
       "4975    China sent mixed signals United States visit S...\n",
       "4976    The United States would consider form peaceful...\n",
       "4977    China said Thursday highest-level U.S. visit t...\n",
       "4978    China Thursday tried play friction United Stat...\n",
       "4979    Russia push expand economic ties China top lev...\n",
       "4980    China gave new hints Monday three-year austeri...\n",
       "4981    China captured big prize recognition battle ri...\n",
       "4982    China preparing lobbying campaign join World T...\n",
       "4983    Asia gave cautious welcome Friday U.S. Preside...\n",
       "4984    China take advantage easing inflation switch f...\n",
       "4985    China Tuesday announced ban poultry poultry pr...\n",
       "4986    China Tuesday announced ban poultry poultry pr...\n",
       "4987    China Tuesday announced ban poultry poultry pr...\n",
       "4988    Two years ago, Chinas second telephone network...\n",
       "4989    Chinas strained ties United States improved dr...\n",
       "4990    China taken cue U.S. Federal Reserve chief Ala...\n",
       "4991    The Stone Group, Chinese high technology compa...\n",
       "4992    China said Thursday strongly opposed visit Jor...\n",
       "4993    A top Chinese defence official stepped reshuff...\n",
       "4994    China warned Monday reinforcing military allia...\n",
       "4995    Chinas central bank chief said inflation would...\n",
       "4996    China ushered 1997, year hailed one significan...\n",
       "4997    China issued tough new rules handling blood pr...\n",
       "4998    China avoid bold moves tackling ailing state e...\n",
       "4999    Communist Party chief Jiang Zemin put personal...\n",
       "Name: Text, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratified train-test split \n",
    "\n",
    "X = df['Text']\n",
    "y = df['Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Obtain word embedding representation of the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using gensim's pre-trained Word2Vec model for obtaining word-embeddings for the text data:\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ' '.join(X_train)\n",
    "more_sentences = ' '.join(X_test)\n",
    "sentences = sentences + more_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences,min_count = 10,window = 3, size = 100) #Taking too long to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = gensim.models.KeyedVectors.load_word2vec_format(\"glove.6B.300d.txt\", binary=False)\n",
    "#Maybe use a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "connectionState = sqlite3.connect('gutenberg/gutenberg.sqlite3')\n",
    "#cursor=connectionState.cursor()\n",
    "data_df = pd.read_sql_query(\"SELECT * FROM articles\", connectionState)\n",
    "# dropping the id column\n",
    "data_df = data_df.drop('id',axis=1)\n",
    "# rename columns to author and text\n",
    "data_df.columns = ['Author','Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Text'] = data_df['Text'].map(lambda x: re.sub('\\r|\\n|\\'','',x))\n",
    "data_df['Text'] = data_df['Text'].map(lambda x: re.sub(r'--\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d','',x))\n",
    "data_df['Text'] = data_df['Text'].map(lambda x: re.sub('\\s+',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_subset = set([x for x in stop_words if 3 <= len(x) <=5])\n",
    "\n",
    "def remove_stopwords(x): #x is the string\n",
    "    word_set = x.split(' ')\n",
    "    for word in word_set:\n",
    "        if word in stop_words:\n",
    "            word_set.remove(word)\n",
    "    return ' '.join(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Text'] = data_df['Text'].map(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Text'] = data_df['Text'].apply(stem_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Text']\n",
    "y = df['Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ' '.join(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences,min_count = 10,window = 3, size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"gutenberg_word2vec_model.pickle\", \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
